{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final assignment\n",
    "\n",
    "Time to condense all you've learnt through this course in a final assignment. This notebook serves as a basic template for you to fill in with the code needed to do what is requested.\n",
    "\n",
    "As always, add as many cells as you need explaining your approach. In particular, **if you do things differently than you did for previous assigments (e.g., because of the feedback received, or because you come up with new ideas), please highlight it, and explain why you changed your mind**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Linear regression for sklearn\n",
    "import sklearn.linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# make the number format not to display in a scientific format\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "## 1. Load data\n",
    "\n",
    "First thing's first. Load the Used Cars Dataset, **using only the columns that you're going to use in your models**. That is, **DON'T read columns that**:\n",
    "* <u>Are obviously useless</u> (for example, unique IDs).\n",
    "\n",
    "* <u>Could be useful, but previous assignments showed you that they're not worth processing for prediction</u>.\n",
    "\n",
    "* <u>Special data types that are different from pure numbers or categories</u> (e.g., geographical or text ones). However, **extra points will be given if you use them, as you've specific material on the subject**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>year</th>\n",
       "      <th>manufacturer</th>\n",
       "      <th>model</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>fuel</th>\n",
       "      <th>odometer</th>\n",
       "      <th>transmission</th>\n",
       "      <th>drive</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>128037</th>\n",
       "      <td>35995</td>\n",
       "      <td>2018.000</td>\n",
       "      <td>acura</td>\n",
       "      <td>tlx 3.5l v6</td>\n",
       "      <td>6 cylinders</td>\n",
       "      <td>gas</td>\n",
       "      <td>22026.000</td>\n",
       "      <td>automatic</td>\n",
       "      <td>fwd</td>\n",
       "      <td>sedan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267336</th>\n",
       "      <td>35990</td>\n",
       "      <td>2018.000</td>\n",
       "      <td>audi</td>\n",
       "      <td>a4 allroad premium plus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gas</td>\n",
       "      <td>29157.000</td>\n",
       "      <td>other</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wagon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312201</th>\n",
       "      <td>54990</td>\n",
       "      <td>2015.000</td>\n",
       "      <td>ram</td>\n",
       "      <td>2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>diesel</td>\n",
       "      <td>68664.000</td>\n",
       "      <td>automatic</td>\n",
       "      <td>4wd</td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371403</th>\n",
       "      <td>15999</td>\n",
       "      <td>2015.000</td>\n",
       "      <td>kia</td>\n",
       "      <td>optima</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gas</td>\n",
       "      <td>68235.000</td>\n",
       "      <td>automatic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73349</th>\n",
       "      <td>0</td>\n",
       "      <td>2018.000</td>\n",
       "      <td>ford</td>\n",
       "      <td>f-150</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gas</td>\n",
       "      <td>53964.000</td>\n",
       "      <td>automatic</td>\n",
       "      <td>4wd</td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31637</th>\n",
       "      <td>8000</td>\n",
       "      <td>2015.000</td>\n",
       "      <td>chevrolet</td>\n",
       "      <td>cruze lt</td>\n",
       "      <td>4 cylinders</td>\n",
       "      <td>gas</td>\n",
       "      <td>75000.000</td>\n",
       "      <td>automatic</td>\n",
       "      <td>fwd</td>\n",
       "      <td>sedan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264939</th>\n",
       "      <td>46875</td>\n",
       "      <td>2015.000</td>\n",
       "      <td>chevrolet</td>\n",
       "      <td>corvette</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gas</td>\n",
       "      <td>29337.000</td>\n",
       "      <td>manual</td>\n",
       "      <td>NaN</td>\n",
       "      <td>coupe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18784</th>\n",
       "      <td>32590</td>\n",
       "      <td>2015.000</td>\n",
       "      <td>mercedes-benz</td>\n",
       "      <td>gla-class gla 45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gas</td>\n",
       "      <td>34811.000</td>\n",
       "      <td>other</td>\n",
       "      <td>NaN</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107796</th>\n",
       "      <td>36500</td>\n",
       "      <td>2018.000</td>\n",
       "      <td>nissan</td>\n",
       "      <td>titan crew cab</td>\n",
       "      <td>8 cylinders</td>\n",
       "      <td>gas</td>\n",
       "      <td>35116.000</td>\n",
       "      <td>automatic</td>\n",
       "      <td>rwd</td>\n",
       "      <td>truck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168121</th>\n",
       "      <td>19590</td>\n",
       "      <td>2012.000</td>\n",
       "      <td>subaru</td>\n",
       "      <td>impreza wrx sedan 4d</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gas</td>\n",
       "      <td>79273.000</td>\n",
       "      <td>other</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sedan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42688 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        price     year   manufacturer                    model    cylinders  \\\n",
       "128037  35995 2018.000          acura              tlx 3.5l v6  6 cylinders   \n",
       "267336  35990 2018.000           audi  a4 allroad premium plus          NaN   \n",
       "312201  54990 2015.000            ram                     2500          NaN   \n",
       "371403  15999 2015.000            kia                   optima          NaN   \n",
       "73349       0 2018.000           ford                    f-150          NaN   \n",
       "...       ...      ...            ...                      ...          ...   \n",
       "31637    8000 2015.000      chevrolet                 cruze lt  4 cylinders   \n",
       "264939  46875 2015.000      chevrolet                 corvette          NaN   \n",
       "18784   32590 2015.000  mercedes-benz         gla-class gla 45          NaN   \n",
       "107796  36500 2018.000         nissan           titan crew cab  8 cylinders   \n",
       "168121  19590 2012.000         subaru     impreza wrx sedan 4d          NaN   \n",
       "\n",
       "          fuel  odometer transmission drive    type  \n",
       "128037     gas 22026.000    automatic   fwd   sedan  \n",
       "267336     gas 29157.000        other   NaN   wagon  \n",
       "312201  diesel 68664.000    automatic   4wd  pickup  \n",
       "371403     gas 68235.000    automatic   NaN   other  \n",
       "73349      gas 53964.000    automatic   4wd  pickup  \n",
       "...        ...       ...          ...   ...     ...  \n",
       "31637      gas 75000.000    automatic   fwd   sedan  \n",
       "264939     gas 29337.000       manual   NaN   coupe  \n",
       "18784      gas 34811.000        other   NaN   other  \n",
       "107796     gas 35116.000    automatic   rwd   truck  \n",
       "168121     gas 79273.000        other   NaN   sedan  \n",
       "\n",
       "[42688 rows x 10 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Your code goes here\n",
    "df = pd.read_csv(\n",
    "    './vehicles.csv', \n",
    "    usecols=['price', 'year', 'manufacturer' ,'cylinders', \n",
    "             'fuel', 'odometer', 'transmission', 'drive', 'type', 'model']\n",
    ")\n",
    "df=df.sample(frac=0.10, replace=True, random_state=1)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Divide into X and Y, and into training and testing set\n",
    "\n",
    "At this point, the dataframe has all features we want in our data, so it's time to split it into training and testing set. Remember some things here:\n",
    "* <u>`Y` is the `price` feature and `X` is all the rest of features</u>.\n",
    "* <u>Use fixed ratios. For example, 80% for training and 20% for testing.</u>\n",
    "* <u>We saw that there's no need to shuffle data, but if you do, justify it, and use always the same `RandomState` so that you always get the same split.</u>\n",
    "\n",
    "**REMARK**: <b><u>don't use test data from now on, till you've tuned your final model! (Step 7 below)</u></b>. Remember: **test data is like the final exam, so we can't access its questions (= test samples) until we've studied (= trained and tuned our model)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code goes here\n",
    "\n",
    "X, Y = df.drop(columns=['price']), df['price']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Remove problematic rows in X_train\n",
    "\n",
    "Now that we have all columns (= features or attributes) needed, it's time to remove rows (= car ads in this case) that we don't want to include. Unfortunately, **scikit-learn doesn't support transformations that remove rows** (there's an <a href=\"https://github.com/scikit-learn/scikit-learn/issues/3855\">open issue</a> about this), so **any row-removing operations must be done as preprocessing steps that cannot be included in a Pipeline**.\n",
    "\n",
    "You know from previous assignments that there're several reasons for which we may want to delete some rows. The most important ones are:\n",
    "* <u>Missing values</u>: there's some feature that has a *NaN/None* value, and we know that:\n",
    "    * <u>The feature is difficult to impute with the rest of information at hand</u> (so we don't know how to replace it with a non-missing value).\n",
    "    * <u>The fact that it's missing isn't relevant for prediction</u> (that is, if we keep it with a NaN value, the model doesn't take that fact into consideration). Note that this only applies to categorical features (as they can be encoded keeping NaNs), but not to numerical ones. **Numerical features cannot be NaN in sklearn models, except for very limited cases**.\n",
    "* <u>Outliers</u>: some value of some feature (or belonging to the target `Y`) isn't missing, but it's wrong, out of bounds, or simply doesn't make sense from the business point of view.\n",
    "\n",
    "**HINT**: once you've decided what rows to remove and with what logic, try to write a function that, given a dataset `X`, returns that same dataset without the rows that meet that logic. Why? Because you'll have to apply this function eventually to `X_test` before predicting for it (see Step 7 below).\n",
    "\n",
    "**REMARK**: why don't we remove problematic rows before splitting into training and testing? Because **that's a subtle form of data leakage**. We simply don't know what will come in `X_test` (actually we do, but we have to behave as if we didn't know!), so we need to infer from `X_train` what is \"problematic\" or an \"outlier\" and what is not. **Note that in previous assignments we cheated a bit because we used the whole dataset to discard prices that are extreme, but now we should do things properly.**\n",
    "\n",
    "We will start here with removing the missing values for the numerical variables which are the year and the odometer. We decided to drop the missing the values because both features cannot be imputed from other features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price          0.000\n",
      "year           0.003\n",
      "transmission   0.005\n",
      "fuel           0.006\n",
      "odometer       0.010\n",
      "model          0.013\n",
      "manufacturer   0.042\n",
      "type           0.221\n",
      "drive          0.307\n",
      "cylinders      0.412\n",
      "Name: % of samples with NAs in each feature, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "### Your code goes here\n",
    "\n",
    "## before we drop any record we will join the X_train and Y_traun together to make sure that we drop the whole record\n",
    "Train_temp = pd.concat([X_train,Y_train], axis = 1)\n",
    "\n",
    "## Now we can drop the missing values\n",
    "#Train_temp = Train_temp.dropna(subset=['year', 'odometer'])  # just drop rows whenever year and/or odometer are missing\n",
    "\n",
    "print(Train_temp.isna().mean().sort_values().rename('% of samples with NAs in each feature'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the transformation part, we will try to impute the manufacturer by utilizing the model. However, If there are records that have missing model and manufacturer, we will drop them. The nxt block will check if we have both model and manufacturer missing and drop the records accordingly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we have a record with a missing manufacturer and model, we will remove it.\n",
    "manufacturer_missing_ind=pd.DataFrame(Train_temp[Train_temp['manufacturer'].isna()].index,columns=['index'])\n",
    "model_missing_ind=pd.DataFrame(Train_temp[Train_temp['model'].isna()].index, columns=['index'])\n",
    "both_missing_ind=pd.merge(manufacturer_missing_ind,model_missing_ind, on='index', how = 'inner')\n",
    "Train_temp.drop(index=both_missing_ind['index'])\n",
    "Train_temp.reset_index(inplace=True, drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tukey's method\n",
    "def tukeys_method(df, variable):\n",
    "    #Takes two parameters: dataframe & variable of interest as string\n",
    "    q1 = df[variable].quantile(0.25)\n",
    "    q3 = df[variable].quantile(0.75)\n",
    "    iqr = q3-q1\n",
    "    inner_fence = 1.5*iqr\n",
    "    outer_fence = 3.0*iqr\n",
    "    \n",
    "    #inner fence lower and upper end\n",
    "    inner_fence_le = q1-inner_fence\n",
    "    inner_fence_ue = q3+inner_fence\n",
    "    \n",
    "    #outer fence lower and upper end\n",
    "    outer_fence_le = q1-outer_fence\n",
    "    outer_fence_ue = q3+outer_fence\n",
    " \n",
    "    for index, x in enumerate(df[variable]):\n",
    "        if x <= inner_fence_le or x >= inner_fence_ue:\n",
    "            #outliers_prob.append(x)\n",
    "            df = df[df[variable] != x]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now let's remove the outliers from the odometer and year\n",
    "Train_temp = tukeys_method(Train_temp,'odometer')\n",
    "Train_temp = tukeys_method(Train_temp,'year')\n",
    "Train_temp = tukeys_method(Train_temp,'price')\n",
    "Train_temp.price.describe()\n",
    "# After droping the problomatic rows we split the X and Y again\n",
    "X_train, Y_train = Train_temp.drop(columns=['price']), Train_temp['price']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "269950"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transform X_train\n",
    "\n",
    "At this point, you know that `X_train` has all necessary features, and you also know that the rows that have remained from the previous steps are the ones you're going to train with.\n",
    "\n",
    "This is the core part. Consider all the features you loaded in point 1, and what you've done so far regarding missing values. **You need to take into consideration these facts**:\n",
    "* <u>Numerical features cannot have missing values</u>.\n",
    "* <u>Unless you use some particular models (e.g., trees), numerical features need to be scaled</u>. **This is particularly important if in step 5 you're using a linear model, or a non-linear model that relies on scalar products** (e.g., SVMs on Neural Networks).\n",
    "* <u>All non-numerical features must be transformed into numbers</u>, so:\n",
    "    * <u>Categorical features should be one-hot encoded</u> (perhaps with NaNs, perhaps without them).\n",
    "    * <u>Ordinal features should be categorized and then one-hot encoded, or transformed into numbers somehow</u>.\n",
    "    * <u>Text features should be tf-idf vectorized</u> (if you use them, and unless you use some more advanced NLP packages).\n",
    "    * <u>Geographical features are tricky</u>. If numerical (such as coordinates), those numbers can only lie in particular ranges. If categorical, usually they follow a hierarchy (for examples regions include states, which include counties). Think carefully about what to do with these!\n",
    "* <u>If you are imputing missing values for some feature, the logic must be included in this step.</u>\n",
    "* <u>If you need to discard some feature because it's used at this step but not anymore, drop it now.</u> For example, if you use `model` to impute missing values in other features, but you don't want to use it for training because it has too many categories.\n",
    "    \n",
    "**HINT**: once you've thought about it, try to condense this into a `ColumnTransformer` that splits processing between different kinds of features. Depending on what you do, it's possible that some of those kind-specific transformations need to be compound too (i.e., not just single transformers but `Pipelines`, `FeatureUnions` or `ColumnTransformers`). Remember about recursion!\n",
    "\n",
    "**HINT**: if you're doing things which aren't included in scikit-learn, such as imputing missing values with some more elaborate logic than replacing by mean or mode (so that `SimpleImputer` isn't enough), you can also **write your own transformers**. To do this, you'll need to write a class that inherits from `BaseEstimator` and `TransformerMixin` and implement yourself the `fit(X)` and the `transform(X)` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline, make_pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "class ExperimentalTransformer(BaseEstimator, TransformerMixin):\n",
    "  def __init__(self):\n",
    "    print('\\n>>>>>>>init() called for ExperimentalTransformer.\\n')\n",
    "\n",
    "  def fit(self, X, y = None):\n",
    "    print('\\n>>>>>>>fit() called for ExperimentalTransformer.\\n')\n",
    "    print(X.columns)\n",
    "    X_ = X.copy()\n",
    "    final_columns=[]\n",
    "    group_list=[]\n",
    "    print(X_.columns)\n",
    "    for col_name in X_.columns:\n",
    "        print('before check column name ' + col_name)\n",
    "        if col_name =='model':\n",
    "            continue\n",
    "        else:\n",
    "            #model_by_manu = X.groupby(['model',col_name], as_index=False).size()\n",
    "            final_columns.append(col_name)\n",
    "            model_by_typ= pd.DataFrame({'size' : X_.groupby( [ 'model',col_name] ).size()}).reset_index()\n",
    "            group_list.append(model_by_typ)\n",
    "        self.model_by_typ=group_list\n",
    "            \n",
    "            #Manu_null=X_[X_[col_name].isna()]\n",
    "            #for i in Manu_null.iterrows():\n",
    "             #   model=Manu_null.at[i[0],'model']\n",
    "                #temp = model_by_typ[model_by_typ.model.apply(lambda x: x == model)]   \n",
    "              #  temp = model_by_typ[model_by_typ['model'] == model]\n",
    "               # print(temp)\n",
    "               # if len(temp) != 0:\n",
    "                #    ind = temp[['size']].idxmax()\n",
    "                 #   X_.at[i[0],col_name] = temp.at[ind[0],col_name]\n",
    "   \n",
    "    return self\n",
    "\n",
    "  def transform(self, X, y = None):\n",
    "    print('\\n>>>>>>>transform() called for ExperimentalTransformer.\\n')\n",
    "    X_ = X.copy()\n",
    "    indicator=0\n",
    "    print('after indicator')\n",
    "    final_columns=[]\n",
    "    print(X_.columns)\n",
    "    for col_name in X_.columns:\n",
    "        print('before check column name ' + col_name)\n",
    "        if col_name =='model':\n",
    "            continue\n",
    "        else:\n",
    "            #model_by_manu = X.groupby(['model',col_name], as_index=False).size()\n",
    "            final_columns.append(col_name)\n",
    "           # model_by_typ= pd.DataFrame({'size' : X_.groupby( [ 'model',col_name] ).size()}).reset_index()\n",
    "            model_by_typ=self.model_by_typ[indicator]\n",
    "            print('start of group by')\n",
    "            print(model_by_typ)\n",
    "            print('end of group by')\n",
    "            Manu_null=X_[X_[col_name].isna()]\n",
    "            for i in Manu_null.iterrows():\n",
    "                model=Manu_null.at[i[0],'model']\n",
    "                #print(model)\n",
    "                #temp = model_by_typ[model_by_typ.model.apply(lambda x: x == model)]   \n",
    "                #temp = model_by_typ[model_by_typ['model'] == model]\n",
    "                if type(model) == str:\n",
    "                    temp= model_by_typ[model_by_typ.model.map(tuple) == tuple(model)]\n",
    "                   # print(temp)\n",
    "                    if len(temp) != 0:\n",
    "                      #  print('value has been replaced' )\n",
    "                        ind = temp[['size']].idxmax()\n",
    "                        X_.at[i[0],col_name] = temp.at[ind[0],col_name]\n",
    "        indicator=indicator+1\n",
    "    print('done for ExperimentalTransformer' +'ind = ')\n",
    "    print(final_columns)\n",
    "    without_model = X_[final_columns]\n",
    "    print(without_model.shape)\n",
    "    print(without_model.dtypes)                \n",
    "    return without_model\n",
    "class ExperimentalTransformer2(BaseEstimator, TransformerMixin):\n",
    "  def __init__(self):\n",
    "    print('\\n>>>>>>>init() called for ExperimentalTransformer2.\\n')\n",
    "\n",
    "  def fit(self, X, y = None):\n",
    "    print('\\n>>>>>>>fit() called for ExperimentalTransformer2.\\n')\n",
    "    #print(X.columns)\n",
    "     \n",
    "    X_ = X.copy() \n",
    "    Y_ = y.copy() \n",
    "    self.avg=y.mean()\n",
    "    df_dict_man_train=[]\n",
    "    counter=0\n",
    "    print(X_.columns)\n",
    "    for col_name in X_.columns:\n",
    "        X_[col_name]= X_[col_name].where(~(X_[col_name].isna()), other='na', inplace=False)\n",
    "        #X_['type']= X_['type'].where(~(X_['type'].isna()), other='na', inplace=False)\n",
    "        a=col_name+'_avg_price'\n",
    "        Train_temp = pd.concat([X_,Y_], axis = 1)\n",
    "\n",
    "        df_man_train = Train_temp.groupby(col_name)[['price']].mean().sort_values(by=['price'], ascending=False).rename(\n",
    "        columns={'price': a})\n",
    "\n",
    "        #df_typ_train = Train_temp.groupby('type')[['price']].mean().sort_values(by=['price'], ascending=False).rename(\n",
    "        #columns={'price': 'typ_avg_price'})\n",
    "\n",
    "# Now we create the dictionary:\n",
    "        df_dict_man_train.insert(counter, df_man_train.to_dict()[a])\n",
    "        #df_dict_man_train[counter] = df_man_train.to_dict()[a]\n",
    "        counter=counter+1\n",
    "    #df_dict_typ_train = df_typ_train.to_dict()['typ_avg_price']\n",
    "    self.df_dict_man_train=df_dict_man_train\n",
    "    #self.df_dict_typ_train=df_dict_typ_train\n",
    "    \n",
    "    return self\n",
    "\n",
    "  def transform(self, X, y = None):\n",
    "    print('\\n>>>>>>>transform() called for ExperimentalTransformer2.\\n')\n",
    "    \n",
    "    X_ = X.copy() \n",
    "# And create a new variable based on each manufacturer's average price:\n",
    "    counter=0\n",
    "    for col_name in X_.columns:\n",
    "     #   print(col_name)\n",
    "        a=col_name+'_avg_price'\n",
    "        X_[col_name] = X_[col_name].replace(self.df_dict_man_train[counter])\n",
    "        # check if manu is new\n",
    "        \n",
    "        X_[col_name] = pd.to_numeric(X_[col_name], errors='coerce')\n",
    "        X_[col_name].fillna(self.avg)\n",
    "        counter=counter+1\n",
    "# Now we can drop the columns\n",
    "       # X_=X_.drop(columns=col_name)\n",
    "    #X_=X_.drop(columns='type')\n",
    "    \n",
    "    #final_columns=[]\n",
    "    #for col_name in X_.columns:\n",
    "     #   print(col_name)\n",
    "      #  if col_name =='model':\n",
    "       #     continue\n",
    "       # else:\n",
    "            #model_by_manu = X.groupby(['model',col_name], as_index=False).size()\n",
    "            \n",
    "            \n",
    "            \n",
    "        #    final_columns.append(col_name)\n",
    "         #   model_by_typ= pd.DataFrame({'size' : X_.groupby( [ 'model',col_name] ).size()}).reset_index()\n",
    "         #   Manu_null=X_[X_[col_name].isna()]\n",
    "          #  for i in Manu_null.iterrows():\n",
    "           #     model=Manu_null.at[i[0],'model']\n",
    "                    \n",
    "            #    temp = model_by_typ[model_by_typ['model'] == model]\n",
    "             #   if len(temp) != 0:\n",
    "              #      ind = temp[['size']].idxmax()\n",
    "               #     X_.at[i[0],col_name] = temp.at[ind[0],col_name]\n",
    "    print(X_.dtypes)\n",
    "    print('done for ExperimentalTransformer2')\n",
    "   # without_model = X_[final_columns]\n",
    "    #((without_model.isnull().sum()/without_model.isnull().count()) * 100).sort_values(ascending=False)\n",
    "    return X_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create pipeline 1\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer2.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "fit pipeline 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "print(\"create pipeline 1\")\n",
    "#categorical_features = ['manufacturer', 'type', 'model','cylinders']\n",
    "categorical_features = ['manufacturer', 'type', 'model']\n",
    "categorical_features1 = ['cylinders', 'model']\n",
    "categorical_features2 = ['transmission', 'drive','fuel']\n",
    "numrical_features1 = ['year', 'odometer']\n",
    "pipe1 = Pipeline(steps=[\n",
    "                       ('fill_data', ExperimentalTransformer()),\n",
    "                      ('convert_with_price', ExperimentalTransformer2()),\n",
    "                      ('imputer1',SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n",
    "                      ('scaler', StandardScaler())\n",
    "                       #('enc', OneHotEncoder(sparse = False, drop ='first'))\n",
    "])\n",
    "pipe2 = Pipeline(steps=[\n",
    "                       ('fill_data_2', ExperimentalTransformer()),\n",
    "                      ('imputer2',SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n",
    "                       ('enc', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "pipe3 = Pipeline(steps=[\n",
    "                      ('imputer3',SimpleImputer(missing_values=np.nan, strategy='median')),\n",
    "                       ('scaler', StandardScaler())\n",
    "])\n",
    "pipe4 = Pipeline(steps=[\n",
    "                       ('imputer4',SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n",
    "                       ('enc2', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', pipe1, categorical_features),\n",
    "        ('cat1', pipe2, categorical_features1),\n",
    "        ('num', pipe3, numrical_features1),\n",
    "        ('cat2', pipe4, categorical_features2),\n",
    "    ],remainder='drop')\n",
    "#new=preprocessor.fit_transform(X_train,Y_train)\n",
    "#clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "#                       ('linear_model', LinearRegression())])\n",
    "\n",
    "print(\"fit pipeline 1\")\n",
    "#print(new)\n",
    "#clf.fit(X_train, Y_train)\n",
    "#preds1 = clf.predict(X_test)\n",
    "#print(preds1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit pipeline LR\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer2.\n",
      "\n",
      "\n",
      ">>>>>>>fit() called for ExperimentalTransformer.\n",
      "\n",
      "Index(['manufacturer', 'type', 'model'], dtype='object')\n",
      "Index(['manufacturer', 'type', 'model'], dtype='object')\n",
      "before check column name manufacturer\n",
      "before check column name type\n",
      "before check column name model\n",
      "\n",
      ">>>>>>>transform() called for ExperimentalTransformer.\n",
      "\n",
      "after indicator\n",
      "Index(['manufacturer', 'type', 'model'], dtype='object')\n",
      "before check column name manufacturer\n",
      "start of group by\n",
      "                          model manufacturer  size\n",
      "0                         (300)     chrysler     1\n",
      "1        (cng) 2500 express van    chevrolet     1\n",
      "2                    - forester       subaru     1\n",
      "3         08' mkz 79,000 miles!      lincoln     1\n",
      "4                      1 series          bmw     1\n",
      "...                         ...          ...   ...\n",
      "5789   z4 sdrive28i roadster 2d          bmw     2\n",
      "5790   z4 sdrive30i roadster 2d          bmw     1\n",
      "5791               z4 sdrive35i          bmw     1\n",
      "5792  z4 sdrive35is roadster 2d          bmw    13\n",
      "5793                        z71    chevrolet     1\n",
      "\n",
      "[5794 rows x 3 columns]\n",
      "end of group by\n",
      "before check column name type\n",
      "start of group by\n",
      "                               model         type  size\n",
      "0                              (300)        other     1\n",
      "1             (cng) 2500 express van        truck     1\n",
      "2     - LUXURY SUV - ALL WHEEL DRIVE        wagon     2\n",
      "3               -2018 Cars and SUV's        sedan     2\n",
      "4                          02 chevy.        truck     1\n",
      "...                              ...          ...   ...\n",
      "6723        z4 sdrive28i roadster 2d        other     2\n",
      "6724        z4 sdrive30i roadster 2d  convertible     1\n",
      "6725                    z4 sdrive35i  convertible     1\n",
      "6726       z4 sdrive35is roadster 2d        other    13\n",
      "6727     ðŸ”¥GMC Sierra 1500 SLEðŸ”¥ 4X4 ðŸ”¥        truck     1\n",
      "\n",
      "[6728 rows x 3 columns]\n",
      "end of group by\n",
      "before check column name model\n",
      "done for ExperimentalTransformerind = \n",
      "['manufacturer', 'type']\n",
      "(32019, 2)\n",
      "manufacturer    object\n",
      "type            object\n",
      "dtype: object\n",
      "\n",
      ">>>>>>>fit() called for ExperimentalTransformer2.\n",
      "\n",
      "Index(['manufacturer', 'type'], dtype='object')\n",
      "\n",
      ">>>>>>>transform() called for ExperimentalTransformer2.\n",
      "\n",
      "manufacturer    float64\n",
      "type            float64\n",
      "dtype: object\n",
      "done for ExperimentalTransformer2\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>fit() called for ExperimentalTransformer.\n",
      "\n",
      "Index(['cylinders', 'model'], dtype='object')\n",
      "Index(['cylinders', 'model'], dtype='object')\n",
      "before check column name cylinders\n",
      "before check column name model\n",
      "\n",
      ">>>>>>>transform() called for ExperimentalTransformer.\n",
      "\n",
      "after indicator\n",
      "Index(['cylinders', 'model'], dtype='object')\n",
      "before check column name cylinders\n",
      "start of group by\n",
      "                               model    cylinders  size\n",
      "0                              (300)  8 cylinders     1\n",
      "1             (cng) 2500 express van  8 cylinders     1\n",
      "2     - LUXURY SUV - ALL WHEEL DRIVE  6 cylinders     2\n",
      "3               -2018 Cars and SUV's  6 cylinders     2\n",
      "4                          02 chevy.  8 cylinders     1\n",
      "...                              ...          ...   ...\n",
      "5082                         z4 3.0i  6 cylinders     1\n",
      "5083                    z4 3.0i 3.0i  6 cylinders     1\n",
      "5084                  z4 convertible  6 cylinders     1\n",
      "5085                    z4 sdrive35i  6 cylinders     1\n",
      "5086     ðŸ”¥GMC Sierra 1500 SLEðŸ”¥ 4X4 ðŸ”¥  8 cylinders     1\n",
      "\n",
      "[5087 rows x 3 columns]\n",
      "end of group by\n",
      "before check column name model\n",
      "done for ExperimentalTransformerind = \n",
      "['cylinders']\n",
      "(32019, 1)\n",
      "cylinders    object\n",
      "dtype: object\n",
      "\n",
      ">>>>>>>transform() called for ExperimentalTransformer.\n",
      "\n",
      "after indicator\n",
      "Index(['manufacturer', 'type', 'model'], dtype='object')\n",
      "before check column name manufacturer\n",
      "start of group by\n",
      "                          model manufacturer  size\n",
      "0                         (300)     chrysler     1\n",
      "1        (cng) 2500 express van    chevrolet     1\n",
      "2                    - forester       subaru     1\n",
      "3         08' mkz 79,000 miles!      lincoln     1\n",
      "4                      1 series          bmw     1\n",
      "...                         ...          ...   ...\n",
      "5789   z4 sdrive28i roadster 2d          bmw     2\n",
      "5790   z4 sdrive30i roadster 2d          bmw     1\n",
      "5791               z4 sdrive35i          bmw     1\n",
      "5792  z4 sdrive35is roadster 2d          bmw    13\n",
      "5793                        z71    chevrolet     1\n",
      "\n",
      "[5794 rows x 3 columns]\n",
      "end of group by\n",
      "before check column name type\n",
      "start of group by\n",
      "                               model         type  size\n",
      "0                              (300)        other     1\n",
      "1             (cng) 2500 express van        truck     1\n",
      "2     - LUXURY SUV - ALL WHEEL DRIVE        wagon     2\n",
      "3               -2018 Cars and SUV's        sedan     2\n",
      "4                          02 chevy.        truck     1\n",
      "...                              ...          ...   ...\n",
      "6723        z4 sdrive28i roadster 2d        other     2\n",
      "6724        z4 sdrive30i roadster 2d  convertible     1\n",
      "6725                    z4 sdrive35i  convertible     1\n",
      "6726       z4 sdrive35is roadster 2d        other    13\n",
      "6727     ðŸ”¥GMC Sierra 1500 SLEðŸ”¥ 4X4 ðŸ”¥        truck     1\n",
      "\n",
      "[6728 rows x 3 columns]\n",
      "end of group by\n",
      "before check column name model\n",
      "done for ExperimentalTransformerind = \n",
      "['manufacturer', 'type']\n",
      "(32019, 2)\n",
      "manufacturer    object\n",
      "type            object\n",
      "dtype: object\n",
      "\n",
      ">>>>>>>transform() called for ExperimentalTransformer2.\n",
      "\n",
      "manufacturer    float64\n",
      "type            float64\n",
      "dtype: object\n",
      "done for ExperimentalTransformer2\n",
      "\n",
      ">>>>>>>transform() called for ExperimentalTransformer.\n",
      "\n",
      "after indicator\n",
      "Index(['cylinders', 'model'], dtype='object')\n",
      "before check column name cylinders\n",
      "start of group by\n",
      "                               model    cylinders  size\n",
      "0                              (300)  8 cylinders     1\n",
      "1             (cng) 2500 express van  8 cylinders     1\n",
      "2     - LUXURY SUV - ALL WHEEL DRIVE  6 cylinders     2\n",
      "3               -2018 Cars and SUV's  6 cylinders     2\n",
      "4                          02 chevy.  8 cylinders     1\n",
      "...                              ...          ...   ...\n",
      "5082                         z4 3.0i  6 cylinders     1\n",
      "5083                    z4 3.0i 3.0i  6 cylinders     1\n",
      "5084                  z4 convertible  6 cylinders     1\n",
      "5085                    z4 sdrive35i  6 cylinders     1\n",
      "5086     ðŸ”¥GMC Sierra 1500 SLEðŸ”¥ 4X4 ðŸ”¥  8 cylinders     1\n",
      "\n",
      "[5087 rows x 3 columns]\n",
      "end of group by\n",
      "before check column name model\n",
      "done for ExperimentalTransformerind = \n",
      "['cylinders']\n",
      "(32019, 1)\n",
      "cylinders    object\n",
      "dtype: object\n",
      "\n",
      "[21140.28656655 29030.07091979 32352.79186786 ... 10153.0552325\n",
      " 31133.17245844 18452.12124466]\n",
      "RMSE: 9657.555287313282\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LRModel = Pipeline(steps=[\n",
    "                        # incorrect column name passed\n",
    "                       ('preprocessor', preprocessor), \n",
    "                       ('linear_model', LinearRegression())\n",
    "])\n",
    "\n",
    "print(\"fit pipeline LR\")\n",
    "LRModel.fit(X_train, Y_train)  \n",
    "preds4 = LRModel.predict(X_train) \n",
    "print(f\"\\n{preds4}\")  # should be [196. 289.]\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(Y_train, preds4))}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer2.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer2.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer2.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer2.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer2.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer2.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer2.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer2.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer2.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer2.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer2.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer2.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer2.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer2.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer2.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer2.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer2.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer2.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer2.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer2.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer2.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer2.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer2.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer2.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer2.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer2.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer2.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer2.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer2.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer2.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer2.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer2.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer2.\n",
      "\n",
      "\n",
      ">>>>>>>init() called for ExperimentalTransformer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn.svm\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "RFR = RandomForestRegressor(max_depth=2, random_state=0)\n",
    "\n",
    "RFRModel = Pipeline(steps=[\n",
    "                        # incorrect column name passed\n",
    "                       ('preprocessor', preprocessor), \n",
    "                       ('RFR', RFR)\n",
    "])\n",
    "'''\n",
    "print(\"fit pipeline LR\")\n",
    "RFRModel.fit(X_train, Y_train)  \n",
    "preds4 = RFRModel.predict(X_train) \n",
    "print(f\"\\n{preds4}\")  # should be [196. 289.]\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(Y_train, preds4))}\\n\")\n",
    "'''\n",
    "param_grid = dict(RFR__max_depth=[2, 3, 4,5],\n",
    "                  RFR__n_estimators=[10, 20,30]\n",
    "                 )\n",
    "\n",
    "grid_search = GridSearchCV(RFRModel, param_grid=param_grid, verbose=10, n_jobs=-1, cv=3)\n",
    "grid_search.fit(X_train, Y_train)\n",
    "print ('best params')\n",
    "print(grid_search.best_estimator_)\n",
    "print ('score')\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "Knn_reg = KNeighborsRegressor(n_neighbors=3)\n",
    "# Grid search result is 10\n",
    "\n",
    "KNNModel = Pipeline(steps=[\n",
    "                        # incorrect column name passed\n",
    "                       ('preprocessor', preprocessor), \n",
    "                       ('KNN',Knn_reg )\n",
    "])\n",
    "'''\n",
    "print(\"fit pipeline LR\")\n",
    "KNNModel.fit(X_train, Y_train)  \n",
    "preds4 = KNNModel.predict(X_train) \n",
    "print(f\"\\n{preds4}\")  # should be [196. 289.]\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(Y_train, preds4))}\\n\")\n",
    "'''\n",
    "param_grid = dict(KNN__n_neighbors=[4, 6, 8,10]\n",
    "                 )\n",
    "\n",
    "grid_search = GridSearchCV(KNNModel, param_grid=param_grid, verbose=10, n_jobs=-1, cv=3)\n",
    "grid_search.fit(X_train, Y_train)\n",
    "print ('best params')\n",
    "print(grid_search.best_estimator_)\n",
    "print ('score')\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_decision_model=DecisionTreeRegressor(max_depth=2,min_samples_leaf=2)\n",
    "\n",
    "DTRModel = Pipeline(steps=[\n",
    "                        # incorrect column name passed\n",
    "                       ('preprocessor', preprocessor), \n",
    "                       ('reg_decision_model',reg_decision_model )\n",
    "])\n",
    "'''\n",
    "print(\"fit pipeline LR\")\n",
    "DTRModel.fit(X_train, Y_train)  \n",
    "preds4 = DTRModel.predict(X_train) \n",
    "print(f\"\\n{preds4}\")  # should be [196. 289.]\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(Y_train, preds4))}\\n\")\n",
    "'''\n",
    "param_grid = dict(reg_decision_model__max_depth=[2, 3, 4,5],\n",
    "                  reg_decision_model__min_samples_leaf=[1, 2,3]\n",
    "                 )\n",
    "\n",
    "grid_search = GridSearchCV(DTRModel, param_grid=param_grid, verbose=10, n_jobs=-1, cv=3)\n",
    "grid_search.fit(X_train, Y_train)\n",
    "print ('best params')\n",
    "print(grid_search.best_estimator_)\n",
    "print ('score')\n",
    "print(grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.svm import SVR\n",
    "SVR_model=SVR(C=1.0, epsilon=0.2,kernel=\"linear\")\n",
    "\n",
    "SVRModel = Pipeline(steps=[\n",
    "                        # incorrect column name passed\n",
    "                       ('preprocessor', preprocessor), \n",
    "                       ('SVR',SVR_model )\n",
    "])\n",
    "'''\n",
    "print(\"fit pipeline LR\")\n",
    "SVRModel.fit(X_train, Y_train)  \n",
    "preds4 = SVRModel.predict(X_train) \n",
    "print(f\"\\n{preds4}\")  # should be [196. 289.]\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(Y_train, preds4))}\\n\")\n",
    "'''\n",
    "\n",
    "param_grid = dict(SVR__kernel=[\"linear\", \"rbf\", \"sigmoid\", \"poly\"],\n",
    "                  SVR__C=[1, 1.5, 2, 2.5, 3]\n",
    "                 )\n",
    "\n",
    "grid_search = GridSearchCV(SVRModel, param_grid=param_grid, verbose=10, n_jobs=-1, cv=3)\n",
    "grid_search.fit(X_train, Y_train)\n",
    "print ('best params')\n",
    "print(grid_search.best_estimator_)\n",
    "print ('score')\n",
    "print(grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.dummy import DummyRegressor\n",
    "DR_model=DummyRegressor(strategy=\"mean\")\n",
    "\n",
    "DRModel = Pipeline(steps=[\n",
    "                        # incorrect column name passed\n",
    "                       ('preprocessor', preprocessor), \n",
    "                       ('DR',DR_model )\n",
    "])\n",
    "\n",
    "print(\"fit pipeline LR\")\n",
    "DRModel.fit(X_train, Y_train)  \n",
    "preds4 = DRModel.predict(X_train) \n",
    "print(f\"\\n{preds4}\")  # should be [196. 289.]\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(Y_train, preds4))}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be removed\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "DR_model=DummyRegressor(strategy=\"mean\")\n",
    "reg_decision_model=DecisionTreeRegressor(max_depth=2,min_samples_leaf=2)\n",
    "Knn_reg = KNeighborsRegressor(n_neighbors=3)\n",
    "BestModel = Pipeline(steps=[\n",
    "                        # incorrect column name passed\n",
    "                       ('preprocessor', preprocessor), \n",
    "                       ('KNN',Knn_reg ), \n",
    "                       ('reg_decision_model',reg_decision_model )\n",
    "])\n",
    "\n",
    "print(\"fit pipeline LR\")\n",
    "DRModel.fit(X_train, Y_train)  \n",
    "preds4 = BestModel.predict(X_train) \n",
    "print(f\"\\n{preds4}\")  # should be [196. 289.]\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(Y_train, preds4))}\\n\")\n",
    "\n",
    "param_grid = dict(reg_decision_model__max_depth=[2, 3, 4,5],\n",
    "                  reg_decision_model__min_samples_leaf=[1, 2,3],\n",
    "                  KNN__n_neighbors=[4, 6, 8,10]\n",
    "                 )\n",
    "\n",
    "grid_search = GridSearchCV(BestModel, param_grid=param_grid, verbose=10, n_jobs=-1, cv=3)\n",
    "grid_search.fit(X_train, Y_train)\n",
    "print ('best estimator')\n",
    "print(grid_search.best_estimator_)\n",
    "print ('score')\n",
    "print(grid_search.best_params_)\n",
    "print ('best params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new=pd.DataFrame(new,columns=['manu','type','cylinders'])\n",
    "((new.isnull().sum()/new.isnull().count()) * 100).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code goes here\n",
    "## Here we are trying to impute the manufacturer based on the model\n",
    "model_by_manu = X_train.groupby(['model','manufacturer'], as_index=False).size()\n",
    "Manu_null=X_train[X_train['manufacturer'].isna()]\n",
    "for i in Manu_null.iterrows():\n",
    "    model=Manu_null.at[i[0],'model']\n",
    "    temp = model_by_manu[model_by_manu['model'] == model]\n",
    "    if len(temp) != 0:\n",
    "        ind = temp[['size']].idxmax()\n",
    "        if (temp[['size']]==temp[['size']].max()).sum()>1:\n",
    "            print('more than one match with max count')\n",
    "        X_train.at[i[0],'manufacturer'] = temp.at[ind[0],'manufacturer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.isna().mean().sort_values().rename('% of samples with NAs in each feature'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## here we will be trying to impute the missing type values similar to the strategy used with the manufacturer\n",
    "## model_by_typ = X_train.groupby(['model','type'], as_index=False).size()\n",
    "\n",
    "model_by_typ= pd.DataFrame({'size' : X_train.groupby( [ 'model','type'] ).size()}).reset_index()\n",
    "Typ_null=X_train[X_train['type'].isna()]\n",
    "for i in Typ_null.iterrows():\n",
    "    model=Typ_null.at[i[0],'model']\n",
    "    print(model_by_typ['model'])\n",
    "    temp = model_by_typ[model_by_typ['model'] == model]\n",
    "    if len(temp) != 0:\n",
    "        ind = temp[['size']].idxmax()\n",
    "        X_train.at[i[0],'type'] = temp.at[ind[0],'type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.isna().mean().sort_values().rename('% of samples with NAs in each feature'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## here we will be trying to impute the missing cylinders similar way\n",
    "model_by_cyl = X_train.groupby(['model','cylinders'], as_index=False).size()\n",
    "Cyl_null=X_train[X_train['cylinders'].isna()]\n",
    "for i in Cyl_null.iterrows():\n",
    "    model=Cyl_null.at[i[0],'model']\n",
    "    temp = model_by_cyl[model_by_cyl['model'] == model]\n",
    "    if len(temp) != 0:\n",
    "        ind = temp[['size']].idxmax()\n",
    "        X_train.at[i[0],'cylinders'] = temp.at[ind[0],'cylinders']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.isna().mean().sort_values().rename('% of samples with NAs in each feature'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the model column is a free text, it is not going to be useful in our model, therefore, se are not going to drop it from our training set. We used it only in the imputaiton part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train.drop(columns=['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the manufacturer & type columns we will transform it by using the average price of each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['manufacturer']= X_train['manufacturer'].where(~(X_train['manufacturer'].isna()), other='na', inplace=False)\n",
    "X_train['type']= X_train['type'].where(~(X_train['type'].isna()), other='na', inplace=False)\n",
    "\n",
    "Train_temp = pd.concat([X_train,Y_train], axis = 1)\n",
    "\n",
    "df_man_train = Train_temp.groupby('manufacturer')[['price']].mean().sort_values(by=['price'], ascending=False).rename(\n",
    "    columns={'price': 'man_avg_price'})\n",
    "\n",
    "df_typ_train = Train_temp.groupby('type')[['price']].mean().sort_values(by=['price'], ascending=False).rename(\n",
    "    columns={'price': 'typ_avg_price'})\n",
    "\n",
    "# Now we create the dictionary:\n",
    "df_dict_man_train = df_man_train.to_dict()['man_avg_price']\n",
    "df_dict_typ_train = df_typ_train.to_dict()['typ_avg_price']\n",
    "\n",
    "# And create a new variable based on each manufacturer's average price:\n",
    "X_train['manufacturer_avg_price'] = X_train['manufacturer'].replace(df_dict_man_train)\n",
    "X_test['manufacturer_avg_price'] = X_test['manufacturer'].replace(df_dict_man_train)\n",
    "\n",
    "X_train['type_avg_price'] = X_train['type'].replace(df_dict_typ_train)\n",
    "X_test['type_avg_price'] = X_test['type'].replace(df_dict_typ_train)\n",
    "\n",
    "# Now we can drop the columns\n",
    "X_train=X_train.drop(columns='manufacturer')\n",
    "X_train=X_train.drop(columns='type')\n",
    "X_test=X_test.drop(columns='manufacturer')\n",
    "X_test=X_test.drop(columns='type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the rest of the columns we will use the one hot encouding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def fun_ohe (df_in, variable):\n",
    "    \n",
    "    ohe = OneHotEncoder(sparse=False, drop='first')\n",
    "    ohe.fit(df_in[[variable]])\n",
    "    ohe_df = pd.DataFrame(ohe.transform(df_in[[variable]]),\n",
    "                 columns = ohe.get_feature_names([variable]))\n",
    "    ohe_df.set_index(df_in.index, inplace=True)\n",
    "    return pd.concat([df_in, ohe_df], axis=1).drop([variable], axis=1)\n",
    "\n",
    "# replace nan with 'na'\n",
    "\n",
    "X_train['condition']= X_train['condition'].where(~(X_train['condition'].isna()), other='na', inplace=False)\n",
    "X_train['cylinders']= X_train['cylinders'].where(~(X_train['cylinders'].isna()), other='na', inplace=False)\n",
    "X_train['transmission']=X_train['transmission'].where(~(X_train['transmission'].isna()), other='na', inplace=False)\n",
    "X_train['fuel']=X_train['fuel'].where(~(X_train['fuel'].isna()), other='na', inplace=False)\n",
    "X_train['title_status']= X_train['title_status'].where(~(X_train['title_status'].isna()), other='na', inplace=False)\n",
    "X_train['drive']= X_train['drive'].where(~(X_train['drive'].isna()), other='na', inplace=False)    \n",
    "    \n",
    "\n",
    "# apply ohe function\n",
    "X_train = fun_ohe(df_in= X_train, variable='cylinders')\n",
    "X_train = fun_ohe(df_in= X_train, variable='fuel')\n",
    "X_train = fun_ohe(df_in= X_train, variable='title_status')\n",
    "X_train = fun_ohe(df_in= X_train, variable='transmission')\n",
    "X_train = fun_ohe(df_in= X_train, variable='condition')\n",
    "X_train = fun_ohe(df_in= X_train, variable='drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize the data\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scale = StandardScaler()\n",
    "X_train_scaled = X_train\n",
    "X_test_scaled = X_test\n",
    "X_train_scaled[['year', 'odometer']] = scale.fit_transform(X_train[['year', 'odometer']])\n",
    "X_test_scaled[['year', 'odometer']] = scale.transform (X_test[['year', 'odometer']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train models\n",
    "\n",
    "Data are now in a suitable way for any model we want to train. Missing values have been dropped or filled, there're no outliers, numbers have been scaled, etc. Try to keep in mind lessons learnt in ML1 and ML2, as to which models may be more suitable for this problem, slower/faster to train, etc.\n",
    "\n",
    "Also **decide on what metric to use to measure performance**; the one you feel more comfortable with, whatever. In any case, follow this motto: \"start simple, and then add complexity little by little\". The usual procedure is:\n",
    "1. <u>Start with a really simple model</u>, perhaps even a `DummyRegressor` (or `DummyClassifier` if this was a classification problem). Such a simple model is very fast to train, and it gives you **a value of the error metric that you must improve. If you do worse than this, you're making some mistake in your pipeline**.\n",
    "2. Once you've that reference dummy performance, <u>turn linear</u>. Use simple linear models, and see where you can get. **The new error should be better than the dummy one, but probably still not very satisfactory**. In any case, **this becomes the new reference to beat**.\n",
    "3. Once you've the reference linear performance, <u>turn non-linear, but interpretable</u>. This is where trees, nearest neighbors or naÃ¯ve bayes come in handy, as they're easily intepreted (if-then rules, using very similar samples, or using independent probabilities). **Most likely you'll get an error which is even better than linear one, so this becomes the new reference**.\n",
    "4. <u>Turn non-linear and non-interpretable</u>. Typically here we use models like SVMs or Neural Networks, which are even more powerful, but harder to train and particularly difficult to explain in simple words.\n",
    "5. If not even all of this is enough, <u>build ensembles</u>. That is, not relying on a single model, but combining what several models say.\n",
    "\n",
    "**HINT**: build a `Pipeline` with the previous preprocessing transformations, and whose final step is the model you want to try. This ensures that transformations are applied before training.\n",
    "\n",
    "**HINT**: use `GridSearchCV/RandomizedSearchCV` to not only try the default model, but also tune its more important hyperparameters. Remember that a `Pipeline` is an estimator, so that's what you feed into the search. Also, remember the double underscore trick to specify that a parameter belongs to the estimator, and also recall the different CV strategies. **If you don't do CV, you'll most likely end up overfitting.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11848064.610319335\n"
     ]
    }
   ],
   "source": [
    "### Using dummy regressor by utilizing the mean stategy\n",
    "dummy_regr = DummyRegressor(strategy=\"mean\")\n",
    "dummy_regr.fit(X_train, Y_train)\n",
    "dummy_regr_pred_train = dummy_regr.predict(X_train)\n",
    "print(mean_squared_error(dummy_regr_pred_train,Y_train,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11846629.811360905\n"
     ]
    }
   ],
   "source": [
    "### Using linear regression\n",
    "Lin_regr = sklearn.linear_model.LinearRegression()\n",
    "Lin_regr.fit(X_train, Y_train)\n",
    "Lin_train_predicted = Lin_regr.predict(X_train)\n",
    "print(mean_squared_error(Lin_train_predicted,Y_train,squared=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10144218.813050075\n"
     ]
    }
   ],
   "source": [
    "### Using KNN with CV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "Knn_reg = KNeighborsRegressor()\n",
    "Knn_grid = {'n_neighbors': [4,5,6,7]}\n",
    "Knn_model = GridSearchCV(Knn_reg, Knn_grid, cv = 5, n_jobs=-1)\n",
    "Knn_model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 5}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Knn_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10144218.813050075\n"
     ]
    }
   ],
   "source": [
    "best_knn = Knn_model.best_estimator_\n",
    "Knn_train_predict = best_knn.predict(X_train)\n",
    "print(mean_squared_error(Knn_train_predict,Y_train,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using Nueral Network\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import preprocessing\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, activation = 'relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(10, activation = 'relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(15, activation = 'relu'))\n",
    "    model.add(Dense(15, activation = 'relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(15, activation = 'relu'))\n",
    "    model.add(Dense(15, activation = 'relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mse', optimizer = 'rmsprop', metrics = ['mae'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KerasRegressor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-dc246bbddbbe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#results = model.fit(X_train, Y_train, epochs=20, batch_size=64)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mNN_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKerasRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuild_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'KerasRegressor' is not defined"
     ]
    }
   ],
   "source": [
    "#results = model.fit(X_train, Y_train, epochs=20, batch_size=64)\n",
    "NN_model = KerasRegressor(build_fn=model,epochs=20, batch_size=64,verbose=0, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan, nan, nan])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "cross_val_score(NN_model,np.array(X_train),np.array(Y_train), scoring=keras.metrics.mean_squared_error, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###HERE ON OUT IS FROM THE NO TRANSFORM VERSION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using Nueral Network\n",
    "### we will try to tune the network with 3,5 or 7 layers using grid search. We will fix the number of nodes to 30\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "NN_reg = MLPRegressor()\n",
    "NN_grid = {'hidden_layer_sizes': [(30,30,30),(30,30,30,30,30),(30,30,30,30,30,30,30)]}\n",
    "NN_model = GridSearchCV(NN_reg, NN_grid,n_jobs = -1,  cv = 5)\n",
    "NN_model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_NN = NN_model.best_estimator_\n",
    "NN_train_predict = best_NN.predict(X_train)\n",
    "print(mean_squared_error(NN_train_predict,Y_train,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using SVM\n",
    "import sklearn.svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "SVR_Grid = {'C' : [1,5,10]}\n",
    "\n",
    "SVR_Reg = sklearn.svm.SVR()\n",
    "\n",
    "SVR_model = GridSearchCV(SVR_Reg,SVR_Grid,n_jobs = -1, cv = 5)\n",
    "\n",
    "SVR_model.fit(X_train,Y_train)\n",
    "\n",
    "best_svr = SVR_model.best_estimator_\n",
    "SVR_train_predict = best_svr.predict(X_train)\n",
    "print(mean_squared_error(SVR_train_predict,Y_train,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVR_model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Decide what the final model would be\n",
    "\n",
    "After having done all the previous steps, you'll have trained several models. Now you need to **decide which of those is the one you're going to choose as your best**. Following the exam metaphor, you have to pick your best student to win the ML Olympics!\n",
    "\n",
    "**HINT**: in order to try different models, you can write an outer loop that tries different estimators, as well as what hyperparameters and values for those hyperparameters are to be tried in the CV search. This loop calls CV search, picks its `best_estimator_` and compares its performance with the best performance you had so far. If the new one is better than your current best, this becomes your new best.\n",
    "\n",
    "**HINT**: it's not always about performance (= score). Sometimes you can improve a bit the score, at the expense of training a model that takes way longer, or that requires much more memory. Besides this, clients usually require some interpretability on the model, so think twice about what \"best\" means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test your final model\n",
    "\n",
    "Time to recover `X_test`, which was put on hold since Step 2. Now you've a final `Pipeline` from Step 6, which knows how to transform the data in `X_test` (Step 4) and knows how to predict (because it's been fit by Step 5).\n",
    "\n",
    "**HINT**: **beware that Step 3 hasn't been applied to the test set!** You need to do that before calling `Pipeline.score(X_test)`. For example, if your model doesn't deal with missing values, you need to remove any row from `X_test` that has missing values! Otherwise the code will crash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. (Optional) Revisit what you've done\n",
    "\n",
    "Once you get the score for `X_test`, it's tempting to try to improve even more. If you got a much worse performance than for `X_train`, chances are that you're overfitting, so you need to refine your CV strategy, use regularization, or choose parameters that don't drive to that (for example, don't let a tree grow without limit!).\n",
    "\n",
    "This is like when you fail an exam, and you want to have another try. The catch is that you already know what the exam is (you saw `X_test`), and you also got your marks (the `score`), so it's not taking another similar exam (as would happen in real life), but taking the same exam again. Strictly speaking, this is another subtle form of data leakage, but a widely used one. The hope is that by refining the training strategy, even if we're cheating a bit, the behavior of the final model when it actually takes another, different exam (that is, when it's put into production), will be better than our current one would have obtained.\n",
    "\n",
    "So we're going to **overlook this fact and allow that, given the results in Step 7, you can go back, try again, change your final estimator in Step 6 and retry Step 7, until you can't get any better**.\n",
    "\n",
    "**HINT**: besides the score, you can also plot your predictions against reality, and try to infer when you predict wrongly. This can give you insights on how to improve the model and/or the pre-processing part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. (Not Optional) Study for your final exam!\n",
    "\n",
    "I hope that this final assignment, together with previous ones, gives you a clear view on how ML must be done in real life. I also hope that it's useful for making up your mind, clarifying concepts and understanding much better all we've seen.\n",
    "\n",
    "All the course slides, notebooks, assignments, feedbacks and forum answers are now your personal `X_train`, your training dataset. So now it's just calling `fit` on yourselves, attending the exam, seeing what the questions in `X_test` are, calling `predict(X_test)` on yourselves (that is, trying to answer correctly all `Y_test`), and getting the highest score possible!\n",
    "\n",
    "Real life is like ML, or ML is like real life, the way you prefer to see it. Thanks for your patience!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
