{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Final assignment\n\nTime to condense all you've learnt through this course in a final assignment. This notebook serves as a basic template for you to fill in with the code needed to do what is requested.\n\nAs always, add as many cells as you need explaining your approach. In particular, **if you do things differently than you did for previous assigments (e.g., because of the feedback received, or because you come up with new ideas), please highlight it, and explain why you changed your mind**.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\n# Linear regression for sklearn\nimport sklearn.linear_model\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# make the number format not to display in a scientific format\npd.set_option('display.float_format', lambda x: '%.3f' % x)\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.tree import DecisionTreeRegressor","metadata":{"execution":{"iopub.status.busy":"2021-08-24T15:22:16.650963Z","iopub.execute_input":"2021-08-24T15:22:16.651600Z","iopub.status.idle":"2021-08-24T15:22:17.983464Z","shell.execute_reply.started":"2021-08-24T15:22:16.651456Z","shell.execute_reply":"2021-08-24T15:22:17.982653Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## 1. Load data\n\nFirst thing's first. Load the Used Cars Dataset, **using only the columns that you're going to use in your models**. That is, **DON'T read columns that**:\n* <u>Are obviously useless</u> (for example, unique IDs).\n\n* <u>Could be useful, but previous assignments showed you that they're not worth processing for prediction</u>.\n\n* <u>Special data types that are different from pure numbers or categories</u> (e.g., geographical or text ones). However, **extra points will be given if you use them, as you've specific material on the subject**.","metadata":{"nbgrader":{"grade":false,"locked":false,"solution":false}}},{"cell_type":"code","source":"### Your code goes here\ndf = pd.read_csv(\n   # './vehicles.csv', \n   '../input/craigslist-carstrucks-data/vehicles.csv',\n    usecols=['price', 'year', 'manufacturer', 'condition', 'cylinders', \n             'fuel', 'odometer', 'title_status', 'transmission', 'drive', \n             'size', 'type', 'model']\n)\ndf\n\n#df=df.sample(frac=0.1, replace=True, random_state=1)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T15:22:17.984592Z","iopub.execute_input":"2021-08-24T15:22:17.984992Z","iopub.status.idle":"2021-08-24T15:22:45.555185Z","shell.execute_reply.started":"2021-08-24T15:22:17.984963Z","shell.execute_reply":"2021-08-24T15:22:45.554417Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"        price     year manufacturer                     model condition  \\\n0        6000      NaN          NaN                       NaN       NaN   \n1       11900      NaN          NaN                       NaN       NaN   \n2       21000      NaN          NaN                       NaN       NaN   \n3        1500      NaN          NaN                       NaN       NaN   \n4        4900      NaN          NaN                       NaN       NaN   \n...       ...      ...          ...                       ...       ...   \n426875  23590 2019.000       nissan         maxima s sedan 4d      good   \n426876  30590 2020.000        volvo  s60 t5 momentum sedan 4d      good   \n426877  34990 2020.000     cadillac          xt4 sport suv 4d      good   \n426878  28990 2018.000        lexus           es 350 sedan 4d      good   \n426879  30590 2019.000          bmw  4 series 430i gran coupe      good   \n\n          cylinders    fuel  odometer title_status transmission drive size  \\\n0               NaN     NaN       NaN          NaN          NaN   NaN  NaN   \n1               NaN     NaN       NaN          NaN          NaN   NaN  NaN   \n2               NaN     NaN       NaN          NaN          NaN   NaN  NaN   \n3               NaN     NaN       NaN          NaN          NaN   NaN  NaN   \n4               NaN     NaN       NaN          NaN          NaN   NaN  NaN   \n...             ...     ...       ...          ...          ...   ...  ...   \n426875  6 cylinders     gas 32226.000        clean        other   fwd  NaN   \n426876          NaN     gas 12029.000        clean        other   fwd  NaN   \n426877          NaN  diesel  4174.000        clean        other   NaN  NaN   \n426878  6 cylinders     gas 30112.000        clean        other   fwd  NaN   \n426879          NaN     gas 22716.000        clean        other   rwd  NaN   \n\n             type  \n0             NaN  \n1             NaN  \n2             NaN  \n3             NaN  \n4             NaN  \n...           ...  \n426875      sedan  \n426876      sedan  \n426877  hatchback  \n426878      sedan  \n426879      coupe  \n\n[426880 rows x 13 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>price</th>\n      <th>year</th>\n      <th>manufacturer</th>\n      <th>model</th>\n      <th>condition</th>\n      <th>cylinders</th>\n      <th>fuel</th>\n      <th>odometer</th>\n      <th>title_status</th>\n      <th>transmission</th>\n      <th>drive</th>\n      <th>size</th>\n      <th>type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>11900</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>21000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1500</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4900</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>426875</th>\n      <td>23590</td>\n      <td>2019.000</td>\n      <td>nissan</td>\n      <td>maxima s sedan 4d</td>\n      <td>good</td>\n      <td>6 cylinders</td>\n      <td>gas</td>\n      <td>32226.000</td>\n      <td>clean</td>\n      <td>other</td>\n      <td>fwd</td>\n      <td>NaN</td>\n      <td>sedan</td>\n    </tr>\n    <tr>\n      <th>426876</th>\n      <td>30590</td>\n      <td>2020.000</td>\n      <td>volvo</td>\n      <td>s60 t5 momentum sedan 4d</td>\n      <td>good</td>\n      <td>NaN</td>\n      <td>gas</td>\n      <td>12029.000</td>\n      <td>clean</td>\n      <td>other</td>\n      <td>fwd</td>\n      <td>NaN</td>\n      <td>sedan</td>\n    </tr>\n    <tr>\n      <th>426877</th>\n      <td>34990</td>\n      <td>2020.000</td>\n      <td>cadillac</td>\n      <td>xt4 sport suv 4d</td>\n      <td>good</td>\n      <td>NaN</td>\n      <td>diesel</td>\n      <td>4174.000</td>\n      <td>clean</td>\n      <td>other</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>hatchback</td>\n    </tr>\n    <tr>\n      <th>426878</th>\n      <td>28990</td>\n      <td>2018.000</td>\n      <td>lexus</td>\n      <td>es 350 sedan 4d</td>\n      <td>good</td>\n      <td>6 cylinders</td>\n      <td>gas</td>\n      <td>30112.000</td>\n      <td>clean</td>\n      <td>other</td>\n      <td>fwd</td>\n      <td>NaN</td>\n      <td>sedan</td>\n    </tr>\n    <tr>\n      <th>426879</th>\n      <td>30590</td>\n      <td>2019.000</td>\n      <td>bmw</td>\n      <td>4 series 430i gran coupe</td>\n      <td>good</td>\n      <td>NaN</td>\n      <td>gas</td>\n      <td>22716.000</td>\n      <td>clean</td>\n      <td>other</td>\n      <td>rwd</td>\n      <td>NaN</td>\n      <td>coupe</td>\n    </tr>\n  </tbody>\n</table>\n<p>426880 rows Ã— 13 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## 2. Divide into X and Y, and into training and testing set\n\nAt this point, the dataframe has all features we want in our data, so it's time to split it into training and testing set. Remember some things here:\n* <u>`Y` is the `price` feature and `X` is all the rest of features</u>.\n* <u>Use fixed ratios. For example, 80% for training and 20% for testing.</u>\n* <u>We saw that there's no need to shuffle data, but if you do, justify it, and use always the same `RandomState` so that you always get the same split.</u>\n\n**REMARK**: <b><u>don't use test data from now on, till you've tuned your final model! (Step 7 below)</u></b>. Remember: **test data is like the final exam, so we can't access its questions (= test samples) until we've studied (= trained and tuned our model)**.","metadata":{}},{"cell_type":"code","source":"### Your code goes here\n\nX, Y = df.drop(columns=['price']), df['price']\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.8, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T15:22:45.556630Z","iopub.execute_input":"2021-08-24T15:22:45.557026Z","iopub.status.idle":"2021-08-24T15:22:45.664172Z","shell.execute_reply.started":"2021-08-24T15:22:45.556995Z","shell.execute_reply":"2021-08-24T15:22:45.663322Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## 3. Remove problematic rows in X_train\n\nNow that we have all columns (= features or attributes) needed, it's time to remove rows (= car ads in this case) that we don't want to include. Unfortunately, **scikit-learn doesn't support transformations that remove rows** (there's an <a href=\"https://github.com/scikit-learn/scikit-learn/issues/3855\">open issue</a> about this), so **any row-removing operations must be done as preprocessing steps that cannot be included in a Pipeline**.\n\nYou know from previous assignments that there're several reasons for which we may want to delete some rows. The most important ones are:\n* <u>Missing values</u>: there's some feature that has a *NaN/None* value, and we know that:\n    * <u>The feature is difficult to impute with the rest of information at hand</u> (so we don't know how to replace it with a non-missing value).\n    * <u>The fact that it's missing isn't relevant for prediction</u> (that is, if we keep it with a NaN value, the model doesn't take that fact into consideration). Note that this only applies to categorical features (as they can be encoded keeping NaNs), but not to numerical ones. **Numerical features cannot be NaN in sklearn models, except for very limited cases**.\n* <u>Outliers</u>: some value of some feature (or belonging to the target `Y`) isn't missing, but it's wrong, out of bounds, or simply doesn't make sense from the business point of view.\n\n**HINT**: once you've decided what rows to remove and with what logic, try to write a function that, given a dataset `X`, returns that same dataset without the rows that meet that logic. Why? Because you'll have to apply this function eventually to `X_test` before predicting for it (see Step 7 below).\n\n**REMARK**: why don't we remove problematic rows before splitting into training and testing? Because **that's a subtle form of data leakage**. We simply don't know what will come in `X_test` (actually we do, but we have to behave as if we didn't know!), so we need to infer from `X_train` what is \"problematic\" or an \"outlier\" and what is not. **Note that in previous assignments we cheated a bit because we used the whole dataset to discard prices that are extreme, but now we should do things properly.**\n\nWe will start here with removing the missing values for the numerical variables which are the year and the odometer. We decided to drop the missing the values because both features cannot be imputed from other features. ","metadata":{}},{"cell_type":"code","source":"### Your code goes here\n\n## before we drop any record we will join the X_train and Y_traun together to make sure that we drop the whole record\nTrain_temp = pd.concat([X_train,Y_train], axis = 1)\n\n## Now we can drop the missing values\nTrain_temp = Train_temp.dropna(subset=['year', 'odometer'])  # just drop rows whenever year and/or odometer are missing\n\nprint(Train_temp.isna().mean().sort_values().rename('% of samples with NAs in each feature'))\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-24T15:22:45.665544Z","iopub.execute_input":"2021-08-24T15:22:45.665945Z","iopub.status.idle":"2021-08-24T15:22:46.210806Z","shell.execute_reply.started":"2021-08-24T15:22:45.665916Z","shell.execute_reply":"2021-08-24T15:22:46.209598Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"year           0.000\nodometer       0.000\nprice          0.000\ntransmission   0.004\nfuel           0.005\nmodel          0.012\ntitle_status   0.017\nmanufacturer   0.039\ntype           0.218\ndrive          0.309\ncondition      0.398\ncylinders      0.413\nsize           0.717\nName: % of samples with NAs in each feature, dtype: float64\n","output_type":"stream"}]},{"cell_type":"markdown","source":"From the above figures, can reialize since more than 70% of the size is missing, therefore, we decided to remove the whole feature.","metadata":{}},{"cell_type":"code","source":"Train_temp = Train_temp.drop(columns=['size'])","metadata":{"execution":{"iopub.status.busy":"2021-08-24T15:22:46.212074Z","iopub.execute_input":"2021-08-24T15:22:46.212373Z","iopub.status.idle":"2021-08-24T15:22:46.253768Z","shell.execute_reply.started":"2021-08-24T15:22:46.212343Z","shell.execute_reply":"2021-08-24T15:22:46.252484Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"In the transformation part, we will try to impute the manufacturer by utilizing the model. However, If there are records that have missing model and manufacturer, we will drop them. The nxt block will check if we have both model and manufacturer missing and drop the records accordingly ","metadata":{}},{"cell_type":"code","source":"# If we have a record with a missing manufacturer and model, we will remove it.\nmanufacturer_missing_ind=pd.DataFrame(Train_temp[Train_temp['manufacturer'].isna()].index,columns=['index'])\nmodel_missing_ind=pd.DataFrame(Train_temp[Train_temp['model'].isna()].index, columns=['index'])\nboth_missing_ind=pd.merge(manufacturer_missing_ind,model_missing_ind, on='index', how = 'inner')\nTrain_temp.drop(index=both_missing_ind['index'])\nTrain_temp.reset_index(inplace=True, drop=True)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-24T15:22:46.255342Z","iopub.execute_input":"2021-08-24T15:22:46.255940Z","iopub.status.idle":"2021-08-24T15:22:46.404990Z","shell.execute_reply.started":"2021-08-24T15:22:46.255891Z","shell.execute_reply":"2021-08-24T15:22:46.403939Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Now we will be removing the outliers for the odometer and year by utilizing the turkey method","metadata":{}},{"cell_type":"code","source":"#Tukey's method\ndef tukeys_method(df, variable):\n    #Takes two parameters: dataframe & variable of interest as string\n    q1 = df[variable].quantile(0.25)\n    q3 = df[variable].quantile(0.75)\n    iqr = q3-q1\n    inner_fence = 1.5*iqr\n    outer_fence = 3*iqr\n    \n    #inner fence lower and upper end\n    inner_fence_le = q1-inner_fence\n    inner_fence_ue = q3+inner_fence\n    \n    #outer fence lower and upper end\n    outer_fence_le = q1-outer_fence\n    outer_fence_ue = q3+outer_fence\n \n    for index, x in enumerate(df[variable]):\n        if x <= outer_fence_le or x >= outer_fence_ue:\n            #outliers_prob.append(x)\n            df = df[df[variable] != x]\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-08-24T15:22:46.406364Z","iopub.execute_input":"2021-08-24T15:22:46.406676Z","iopub.status.idle":"2021-08-24T15:22:46.414026Z","shell.execute_reply.started":"2021-08-24T15:22:46.406646Z","shell.execute_reply":"2021-08-24T15:22:46.413158Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"\n# Now let's remove the outliers from the odometer and year\nTrain_temp = tukeys_method(Train_temp,'odometer')\nTrain_temp = tukeys_method(Train_temp,'year')\nTrain_temp = tukeys_method(Train_temp,'price')\n\n# After droping the problomatic rows we split the X and Y again\nX_train, Y_train = Train_temp.drop(columns=['price']), Train_temp['price']\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Transform X_train\n\nAt this point, you know that `X_train` has all necessary features, and you also know that the rows that have remained from the previous steps are the ones you're going to train with.\n\nThis is the core part. Consider all the features you loaded in point 1, and what you've done so far regarding missing values. **You need to take into consideration these facts**:\n* <u>Numerical features cannot have missing values</u>.\n* <u>Unless you use some particular models (e.g., trees), numerical features need to be scaled</u>. **This is particularly important if in step 5 you're using a linear model, or a non-linear model that relies on scalar products** (e.g., SVMs on Neural Networks).\n* <u>All non-numerical features must be transformed into numbers</u>, so:\n    * <u>Categorical features should be one-hot encoded</u> (perhaps with NaNs, perhaps without them).\n    * <u>Ordinal features should be categorized and then one-hot encoded, or transformed into numbers somehow</u>.\n    * <u>Text features should be tf-idf vectorized</u> (if you use them, and unless you use some more advanced NLP packages).\n    * <u>Geographical features are tricky</u>. If numerical (such as coordinates), those numbers can only lie in particular ranges. If categorical, usually they follow a hierarchy (for examples regions include states, which include counties). Think carefully about what to do with these!\n* <u>If you are imputing missing values for some feature, the logic must be included in this step.</u>\n* <u>If you need to discard some feature because it's used at this step but not anymore, drop it now.</u> For example, if you use `model` to impute missing values in other features, but you don't want to use it for training because it has too many categories.\n    \n**HINT**: once you've thought about it, try to condense this into a `ColumnTransformer` that splits processing between different kinds of features. Depending on what you do, it's possible that some of those kind-specific transformations need to be compound too (i.e., not just single transformers but `Pipelines`, `FeatureUnions` or `ColumnTransformers`). Remember about recursion!\n\n**HINT**: if you're doing things which aren't included in scikit-learn, such as imputing missing values with some more elaborate logic than replacing by mean or mode (so that `SimpleImputer` isn't enough), you can also **write your own transformers**. To do this, you'll need to write a class that inherits from `BaseEstimator` and `TransformerMixin` and implement yourself the `fit(X)` and the `transform(X)` methods.","metadata":{}},{"cell_type":"code","source":"### Your code goes here\n## Here we are trying to impute the manufacturer based on the model\nmodel_by_manu = X_train.groupby(['model','manufacturer'], as_index=False).size()\nManu_null=X_train[X_train['manufacturer'].isna()]\nfor i in Manu_null.iterrows():\n    model=Manu_null.at[i[0],'model']\n    temp = model_by_manu[model_by_manu['model'] == model]\n    if len(temp) != 0:\n        ind = temp[['size']].idxmax()\n        X_train.at[i[0],'manufacturer'] = temp.at[ind[0],'manufacturer']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train.isna().mean().sort_values().rename('% of samples with NAs in each feature'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## here we will be trying to impute the missing type values similar to the strategy used with the manufacturer\nmodel_by_typ = X_train.groupby(['model','type'], as_index=False).size()\nTyp_null=X_train[X_train['type'].isna()]\nfor i in Typ_null.iterrows():\n    model=Typ_null.at[i[0],'model']\n    temp = model_by_typ[model_by_typ['model'] == model]\n    if len(temp) != 0:\n        ind = temp[['size']].idxmax()\n        X_train.at[i[0],'type'] = temp.at[ind[0],'type']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train.isna().mean().sort_values().rename('% of samples with NAs in each feature'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## here we will be trying to impute the missing cylinders similar way\nmodel_by_cyl = X_train.groupby(['model','cylinders'], as_index=False).size()\nCyl_null=X_train[X_train['cylinders'].isna()]\nfor i in Cyl_null.iterrows():\n    model=Cyl_null.at[i[0],'model']\n    temp = model_by_cyl[model_by_cyl['model'] == model]\n    if len(temp) != 0:\n        ind = temp[['size']].idxmax()\n        X_train.at[i[0],'cylinders'] = temp.at[ind[0],'cylinders']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train.isna().mean().sort_values().rename('% of samples with NAs in each feature'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since the model column is a free text, it is not going to be useful in our model, therefore, se are not going to drop it from our training set. We used it only in the imputaiton part.","metadata":{}},{"cell_type":"code","source":"X_train=X_train.drop(columns=['model'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For the manufacturer & type columns we will transform it by using the average price of each one.","metadata":{}},{"cell_type":"code","source":"X_train['manufacturer']= X_train['manufacturer'].where(~(X_train['manufacturer'].isna()), other='na', inplace=False)\nX_train['type']= X_train['type'].where(~(X_train['type'].isna()), other='na', inplace=False)\n\nTrain_temp = pd.concat([X_train,Y_train], axis = 1)\n\ndf_man_train = Train_temp.groupby('manufacturer')[['price']].mean().sort_values(by=['price'], ascending=False).rename(\n    columns={'price': 'man_avg_price'})\n\ndf_typ_train = Train_temp.groupby('type')[['price']].mean().sort_values(by=['price'], ascending=False).rename(\n    columns={'price': 'typ_avg_price'})\n\n# Now we create the dictionary:\ndf_dict_man_train = df_man_train.to_dict()['man_avg_price']\ndf_dict_typ_train = df_typ_train.to_dict()['typ_avg_price']\n\n# And create a new variable based on each manufacturer's average price:\nX_train['manufacturer_avg_price'] = X_train['manufacturer'].replace(df_dict_man_train)\nX_test['manufacturer_avg_price'] = X_test['manufacturer'].replace(df_dict_man_train)\n\nX_train['type_avg_price'] = X_train['type'].replace(df_dict_typ_train)\nX_test['type_avg_price'] = X_test['type'].replace(df_dict_typ_train)\n\n# Now we can drop the columns\nX_train=X_train.drop(columns='manufacturer')\nX_train=X_train.drop(columns='type')\nX_test=X_test.drop(columns='manufacturer')\nX_test=X_test.drop(columns='type')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For the rest of the columns we will use the one hot encouding","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\ndef fun_ohe (df_in, variable):\n    \n    ohe = OneHotEncoder(sparse=False, drop='first')\n    ohe.fit(df_in[[variable]])\n    ohe_df = pd.DataFrame(ohe.transform(df_in[[variable]]),\n                 columns = ohe.get_feature_names([variable]))\n    ohe_df.set_index(df_in.index, inplace=True)\n    return pd.concat([df_in, ohe_df], axis=1).drop([variable], axis=1)\n\n# replace nan with 'na'\n\nX_train['condition']= X_train['condition'].where(~(X_train['condition'].isna()), other='na', inplace=False)\nX_train['cylinders']= X_train['cylinders'].where(~(X_train['cylinders'].isna()), other='na', inplace=False)\nX_train['transmission']=X_train['transmission'].where(~(X_train['transmission'].isna()), other='na', inplace=False)\nX_train['fuel']=X_train['fuel'].where(~(X_train['fuel'].isna()), other='na', inplace=False)\nX_train['title_status']= X_train['title_status'].where(~(X_train['title_status'].isna()), other='na', inplace=False)\nX_train['drive']= X_train['drive'].where(~(X_train['drive'].isna()), other='na', inplace=False)    \n    \n\n# apply ohe function\nX_train = fun_ohe(df_in= X_train, variable='cylinders')\nX_train = fun_ohe(df_in= X_train, variable='fuel')\nX_train = fun_ohe(df_in= X_train, variable='title_status')\nX_train = fun_ohe(df_in= X_train, variable='transmission')\nX_train = fun_ohe(df_in= X_train, variable='condition')\nX_train = fun_ohe(df_in= X_train, variable='drive')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Train models\n\nData are now in a suitable way for any model we want to train. Missing values have been dropped or filled, there're no outliers, numbers have been scaled, etc. Try to keep in mind lessons learnt in ML1 and ML2, as to which models may be more suitable for this problem, slower/faster to train, etc.\n\nAlso **decide on what metric to use to measure performance**; the one you feel more comfortable with, whatever. In any case, follow this motto: \"start simple, and then add complexity little by little\". The usual procedure is:\n1. <u>Start with a really simple model</u>, perhaps even a `DummyRegressor` (or `DummyClassifier` if this was a classification problem). Such a simple model is very fast to train, and it gives you **a value of the error metric that you must improve. If you do worse than this, you're making some mistake in your pipeline**.\n2. Once you've that reference dummy performance, <u>turn linear</u>. Use simple linear models, and see where you can get. **The new error should be better than the dummy one, but probably still not very satisfactory**. In any case, **this becomes the new reference to beat**.\n3. Once you've the reference linear performance, <u>turn non-linear, but interpretable</u>. This is where trees, nearest neighbors or naÃ¯ve bayes come in handy, as they're easily intepreted (if-then rules, using very similar samples, or using independent probabilities). **Most likely you'll get an error which is even better than linear one, so this becomes the new reference**.\n4. <u>Turn non-linear and non-interpretable</u>. Typically here we use models like SVMs or Neural Networks, which are even more powerful, but harder to train and particularly difficult to explain in simple words.\n5. If not even all of this is enough, <u>build ensembles</u>. That is, not relying on a single model, but combining what several models say.\n\n**HINT**: build a `Pipeline` with the previous preprocessing transformations, and whose final step is the model you want to try. This ensures that transformations are applied before training.\n\n**HINT**: use `GridSearchCV/RandomizedSearchCV` to not only try the default model, but also tune its more important hyperparameters. Remember that a `Pipeline` is an estimator, so that's what you feed into the search. Also, remember the double underscore trick to specify that a parameter belongs to the estimator, and also recall the different CV strategies. **If you don't do CV, you'll most likely end up overfitting.**","metadata":{}},{"cell_type":"code","source":"### Using dummy regressor by utilizing the mean stategy\ndummy_regr = DummyRegressor(strategy=\"mean\")\ndummy_regr.fit(X_train, Y_train)\ndummy_regr_pred_train = dummy_regr.predict(X_train)\nprint(mean_squared_error(dummy_regr_pred_train,Y_train,squared=False))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Using linear regression\nLin_regr = sklearn.linear_model.LinearRegression(n_jobs=-1)\nLin_regr.fit(X_train, Y_train)\nLin_train_predicted = Lin_regr.predict(X_train)\nprint(mean_squared_error(Lin_train_predicted,Y_train,squared=False))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Using KNN with CV\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.model_selection import GridSearchCV\nKnn_reg = KNeighborsRegressor(n_jobs = -1)\nKnn_grid = {'n_neighbors': [4,5,6,7]}\nKnn_model = GridSearchCV(Knn_reg, Knn_grid,n_jobs = -1,  cv = 5)\nKnn_model.fit(X_train, Y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Knn_model.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_knn = Knn_model.best_estimator_\nKnn_train_predict = best_knn.predict(X_train)\nprint(mean_squared_error(Knn_train_predict,Y_train,squared=False))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Using Nueral Network\n### we will try to tune the network with 3,5 or 7 layers using grid search. We will fix the number of nodes to 30\n\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.model_selection import GridSearchCV\nNN_reg = MLPRegressor()\nNN_grid = {'hidden_layer_sizes': [(30,30,30),(30,30,30,30,30),(30,30,30,30,30,30,30)]}\nNN_model = GridSearchCV(NN_reg, NN_grid,n_jobs = -1,  cv = 5)\nNN_model.fit(X_train, Y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NN_model.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_NN = NN_model.best_estimator_\nNN_train_predict = best_NN.predict(X_train)\nprint(mean_squared_error(NN_train_predict,Y_train,squared=False))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using SVM\nimport sklearn.svm\nfrom sklearn.model_selection import GridSearchCV\n\nSVR_Grid = {'C' : [1,5,10]}\n\nSVR_Reg = sklearn.svm.SVR()\n\nSVR_model = GridSearchCV(SVR_Reg,SVR_Grid,n_jobs = -1, cv = 5)\n\nSVR_model.fit(X_train,Y_train)\n\nbest_svr = SVR_model.best_estimator_\nSVR_train_predict = best_svr.predict(X_train)\nprint(mean_squared_error(SVR_train_predict,Y_train,squared=False))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SVR_model.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. Decide what the final model would be\n\nAfter having done all the previous steps, you'll have trained several models. Now you need to **decide which of those is the one you're going to choose as your best**. Following the exam metaphor, you have to pick your best student to win the ML Olympics!\n\n**HINT**: in order to try different models, you can write an outer loop that tries different estimators, as well as what hyperparameters and values for those hyperparameters are to be tried in the CV search. This loop calls CV search, picks its `best_estimator_` and compares its performance with the best performance you had so far. If the new one is better than your current best, this becomes your new best.\n\n**HINT**: it's not always about performance (= score). Sometimes you can improve a bit the score, at the expense of training a model that takes way longer, or that requires much more memory. Besides this, clients usually require some interpretability on the model, so think twice about what \"best\" means.","metadata":{}},{"cell_type":"code","source":"### Your code goes here\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7. Test your final model\n\nTime to recover `X_test`, which was put on hold since Step 2. Now you've a final `Pipeline` from Step 6, which knows how to transform the data in `X_test` (Step 4) and knows how to predict (because it's been fit by Step 5).\n\n**HINT**: **beware that Step 3 hasn't been applied to the test set!** You need to do that before calling `Pipeline.score(X_test)`. For example, if your model doesn't deal with missing values, you need to remove any row from `X_test` that has missing values! Otherwise the code will crash.","metadata":{}},{"cell_type":"code","source":"### Your code goes here","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8. (Optional) Revisit what you've done\n\nOnce you get the score for `X_test`, it's tempting to try to improve even more. If you got a much worse performance than for `X_train`, chances are that you're overfitting, so you need to refine your CV strategy, use regularization, or choose parameters that don't drive to that (for example, don't let a tree grow without limit!).\n\nThis is like when you fail an exam, and you want to have another try. The catch is that you already know what the exam is (you saw `X_test`), and you also got your marks (the `score`), so it's not taking another similar exam (as would happen in real life), but taking the same exam again. Strictly speaking, this is another subtle form of data leakage, but a widely used one. The hope is that by refining the training strategy, even if we're cheating a bit, the behavior of the final model when it actually takes another, different exam (that is, when it's put into production), will be better than our current one would have obtained.\n\nSo we're going to **overlook this fact and allow that, given the results in Step 7, you can go back, try again, change your final estimator in Step 6 and retry Step 7, until you can't get any better**.\n\n**HINT**: besides the score, you can also plot your predictions against reality, and try to infer when you predict wrongly. This can give you insights on how to improve the model and/or the pre-processing part.","metadata":{}},{"cell_type":"code","source":"### Your code goes here","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 9. (Not Optional) Study for your final exam!\n\nI hope that this final assignment, together with previous ones, gives you a clear view on how ML must be done in real life. I also hope that it's useful for making up your mind, clarifying concepts and understanding much better all we've seen.\n\nAll the course slides, notebooks, assignments, feedbacks and forum answers are now your personal `X_train`, your training dataset. So now it's just calling `fit` on yourselves, attending the exam, seeing what the questions in `X_test` are, calling `predict(X_test)` on yourselves (that is, trying to answer correctly all `Y_test`), and getting the highest score possible!\n\nReal life is like ML, or ML is like real life, the way you prefer to see it. Thanks for your patience!","metadata":{}}]}